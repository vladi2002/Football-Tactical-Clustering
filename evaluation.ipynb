{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed516db4",
   "metadata": {},
   "source": [
    "Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a0fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5c6376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "{'event_count': 28, 'most_common_event': 'PASS', 'unique_players': 5, 'zone_name': 'OUTSIDE', 'role_distribution': array([0.2, 0.2, 0.6, 0. ])}\n",
      "293\n",
      "{'weight': 9, 'transition_frequency': 0.4090909090909091, 'most_common_event': 'PASS', 'start_zone_name': 'LEFT_WING_MID_THIRD_ATT', 'end_zone_name': 'LEFT_HALF_MID_THIRD_ATT'}\n"
     ]
    }
   ],
   "source": [
    "graph=pickle.load(open('./graphs/graph_match2499719_team1609.pkl','rb'))\n",
    "print(len(graph.nodes))\n",
    "print(graph.nodes[40])\n",
    "print(len(graph.edges))\n",
    "print(graph.edges[25, 26])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb22906",
   "metadata": {},
   "source": [
    "We have to encode all event types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5188e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_event_semantic(event_type):\n",
    "    \"\"\"Group events by semantic meaning\"\"\"\n",
    "    # Group related events\n",
    "    attacking_events = ['PASS', 'SHOT', 'CARRY', 'TAKE_ON']\n",
    "    defensive_events = ['CLEARANCE', 'INTERCEPTION', 'PRESSURE', 'RECOVERY', 'DUEL']\n",
    "    goalkeeper_events = ['GOALKEEPER']\n",
    "    disruption_events = ['FOUL_COMMITTED', 'CARD', 'MISCONTROL', 'BALL_OUT']\n",
    "    meta_events = ['FORMATION_CHANGE', 'SUBSTITUTION', 'PLAYER_ON', 'PLAYER_OFF']\n",
    "    \n",
    "    if event_type in attacking_events:\n",
    "        return [1, 0, 0, 0, 0, 0]  # Attacking\n",
    "    elif event_type in defensive_events:\n",
    "        return [0, 1, 0, 0, 0, 0]  # Defensive\n",
    "    elif event_type in goalkeeper_events:\n",
    "        return [0, 0, 1, 0, 0, 0]  # Goalkeeper\n",
    "    elif event_type in disruption_events:\n",
    "        return [0, 0, 0, 1, 0, 0]  # Disruption\n",
    "    elif event_type in meta_events:\n",
    "        return [0, 0, 0, 0, 1, 0]  # Meta\n",
    "    else:\n",
    "        return [0, 0, 0, 0, 0, 1]  # Generic/Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de3d33e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Portable\\ProgramsPortable\\Anaconda\\envs\\graphml\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from spectral_build_vizualization import discover_tactical_patterns\n",
    "\n",
    "labels, nodes = discover_tactical_patterns(graph, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60b8e2",
   "metadata": {},
   "source": [
    "## Silhouette score\n",
    "\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette value ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6289fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08182848837817866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "node_features = []\n",
    "for node in graph.nodes():\n",
    "    \n",
    "    node_data = graph.nodes[node]\n",
    "    \n",
    "    # Create feature vector from the node attributes\n",
    "    features = [\n",
    "        node_data['event_count'],\n",
    "        node_data['unique_players'],\n",
    "        *encode_event_semantic(node_data['most_common_event']),\n",
    "        *node_data['role_distribution']  # This unpacks the 4 values\n",
    "    ]\n",
    "    node_features.append(features)\n",
    "\n",
    "node_features = np.array(node_features)\n",
    "# Standardize features so as to have mean=0 and variance=1 \n",
    "# Otherwise, features with larger scales can dominate the distance calculations\n",
    "node_features = scalar.fit_transform(node_features)\n",
    "score = silhouette_score(node_features, np.array(labels))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d2ed3",
   "metadata": {},
   "source": [
    "## Calinski–Harabasz (CH) Index\n",
    "Defined as the ratio of the between-cluster separation (BCSS) to the within-cluster dispersion (WCSS), normalized by their number of degrees of freedom. BCSS (Between-Cluster Sum of Squares) is the weighted sum of squared Euclidean distances between each cluster centroid (mean) and the overall data centroid (mean). WCSS (Within-Cluster Sum of Squares) is the sum of squared Euclidean distances between the data points and their respective cluster centroids.\n",
    "\n",
    "- < 10: Very poor separation\n",
    "- 10-50: Poor to moderate separation\n",
    "- 50-100: Moderate separation\n",
    "- 100+: Good separation\n",
    "- 1000+: Excellent separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09a1ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.519115642133684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Portable\\ProgramsPortable\\Anaconda\\envs\\graphml\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "labels, nodes = discover_tactical_patterns(graph, k=4)\n",
    "node_features = []\n",
    "for node in graph.nodes():\n",
    "    \n",
    "    node_data = graph.nodes[node]\n",
    "    \n",
    "    # Create feature vector from the node attributes\n",
    "    features = [\n",
    "        node_data['event_count'],\n",
    "        node_data['unique_players'],\n",
    "        *encode_event_semantic(node_data['most_common_event']),\n",
    "        *node_data['role_distribution']  # This unpacks the 4 values\n",
    "    ]\n",
    "    node_features.append(features)\n",
    "\n",
    "node_features = np.array(node_features)\n",
    "node_features = scalar.fit_transform(node_features)\n",
    "ch_score = calinski_harabasz_score(node_features, np.array(labels))\n",
    "print(ch_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6b53b",
   "metadata": {},
   "source": [
    "## Davies Bouldin Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd94976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1881962153032823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Portable\\ProgramsPortable\\Anaconda\\envs\\graphml\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "labels, nodes = discover_tactical_patterns(graph, k=4)\n",
    "node_features = []\n",
    "for node in graph.nodes():\n",
    "    \n",
    "    node_data = graph.nodes[node]\n",
    "    \n",
    "    # Create feature vector from the node attributes\n",
    "    features = [\n",
    "        node_data['event_count'],\n",
    "        node_data['unique_players'],\n",
    "        *encode_event_semantic(node_data['most_common_event']),\n",
    "        *node_data['role_distribution']  # This unpacks the 4 values\n",
    "    ]\n",
    "    node_features.append(features)\n",
    "\n",
    "node_features = np.array(node_features)\n",
    "node_features = scalar.fit_transform(node_features)\n",
    "db_score = davies_bouldin_score(node_features, np.array(labels))\n",
    "print(db_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b55e9",
   "metadata": {},
   "source": [
    "## ANOVA (Analysis of Variance) \n",
    "Tests whether cluster means differ significantly across features. It tells you if your clusters are actually separating the data in meaningful ways. What ANOVA Tells You:\n",
    "\n",
    "- High F-statistic, Low p-value (<0.05): Feature significantly differs between clusters\n",
    "- Low F-statistic, High p-value (>0.05): Feature doesn't help distinguish clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20658747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Feature  F-statistic      p-value Significant\n",
      "unique_players    17.684085 3.705726e-07         Yes\n",
      "   event_count    14.670960 2.380039e-06         Yes\n",
      "    semantic_0     2.852519 5.119929e-02          No\n",
      "    semantic_1     2.852519 5.119929e-02          No\n",
      "        role_2     2.656236 6.351865e-02          No\n",
      "        role_1     0.877082 4.623076e-01          No\n",
      "        role_0     0.512427 6.763652e-01          No\n",
      "        role_3     0.046575 9.864417e-01          No\n",
      "    semantic_2          NaN          NaN          No\n",
      "    semantic_3          NaN          NaN          No\n",
      "    semantic_4          NaN          NaN          No\n",
      "    semantic_5          NaN          NaN          No\n",
      "\n",
      "2/12 features significantly differ between clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Portable\\ProgramsPortable\\Anaconda\\envs\\graphml\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Portable\\ProgramsPortable\\Anaconda\\envs\\graphml\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:579: ConstantInputWarning: Each of the input arrays is constant; the F statistic is not defined or infinite\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "labels, nodes = discover_tactical_patterns(graph, k=4)\n",
    "node_features = []\n",
    "for node in graph.nodes():\n",
    "    \n",
    "    node_data = graph.nodes[node]\n",
    "    \n",
    "    # Create feature vector from the node attributes\n",
    "    features = [\n",
    "        node_data['event_count'],\n",
    "        node_data['unique_players'],\n",
    "        *encode_event_semantic(node_data['most_common_event']),\n",
    "        *node_data['role_distribution']  # This unpacks the 4 values\n",
    "    ]\n",
    "    node_features.append(features)\n",
    "\n",
    "node_features = np.array(node_features)\n",
    "node_features = scalar.fit_transform(node_features)\n",
    "\n",
    "def anova_test_clustering(features, labels, feature_names=None):\n",
    "    \"\"\"\n",
    "    Perform ANOVA test for each feature across clusters\n",
    "    \n",
    "    Args:\n",
    "        features: numpy array (n_samples, n_features)\n",
    "        labels: cluster labels\n",
    "        feature_names: list of feature names\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with F-statistics and p-values\n",
    "    \"\"\"\n",
    "    n_features = features.shape[1]\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature_{i}' for i in range(n_features)]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        # Split feature values by cluster\n",
    "        groups = [features[labels == cluster, i] for cluster in np.unique(labels)]\n",
    "        \n",
    "        # Perform one-way ANOVA\n",
    "        f_stat, p_value = stats.f_oneway(*groups)\n",
    "        \n",
    "        results.append({\n",
    "            'Feature': feature_names[i],\n",
    "            'F-statistic': f_stat,\n",
    "            'p-value': p_value,\n",
    "            'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('F-statistic', ascending=False)\n",
    "\n",
    "feature_names = (\n",
    "    ['event_count', 'unique_players'] + \n",
    "    [f'semantic_{i}' for i in range(6)] + \n",
    "    [f'role_{i}' for i in range(4)]\n",
    ")\n",
    "\n",
    "anova_results = anova_test_clustering(node_features, labels, feature_names)\n",
    "print(anova_results.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "significant_features = anova_results[anova_results['Significant'] == 'Yes']\n",
    "print(f\"\\n{len(significant_features)}/{len(feature_names)} features significantly differ between clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5e37f",
   "metadata": {},
   "source": [
    "## Modularity\n",
    "\n",
    "Modularity measures how well a network is divided into communities/clusters. It compares the number of edges within clusters vs what you'd expect in a random network.\n",
    "\n",
    "Range: -1 to 1\n",
    "\n",
    "- \\> 0.3: Good community structure\n",
    "- \\> 0.5: Strong community structure\n",
    "- \\> 0.7: Very strong community structure\n",
    "- < 0.3: Weak or no community structure\n",
    "Negative: Worse than random\n",
    "\n",
    "Key difference: Unlike Silhouette/CH, modularity uses the graph structure (edges), not just node features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd63640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modularity Score: 0.32949708034846625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Portable\\ProgramsPortable\\Anaconda\\envs\\graphml\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "labels, nodes = discover_tactical_patterns(graph, k=4)\n",
    "\n",
    "def calculate_modularity(graph, labels):\n",
    "    \"\"\"\n",
    "    Calculate modularity for a clustering on a graph\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX graph (DiGraph or Graph)\n",
    "        labels: cluster assignment for each node\n",
    "    \n",
    "    Returns:\n",
    "        modularity score\n",
    "    \"\"\"\n",
    "    # Create communities from labels\n",
    "    # NetworkX expects a list of sets, one set per community\n",
    "    unique_labels = np.unique(labels)\n",
    "    node_list = list(graph.nodes())\n",
    "    \n",
    "    communities = []\n",
    "    for label in unique_labels:\n",
    "        # Get nodes in this cluster\n",
    "        cluster_nodes = [node_list[i] for i, l in enumerate(labels) if l == label]\n",
    "        communities.append(set(cluster_nodes))\n",
    "    \n",
    "    # Calculate modularity\n",
    "    modularity = nx.community.modularity(graph, communities)\n",
    "    \n",
    "    return modularity\n",
    "\n",
    "modularity_score = calculate_modularity(graph, labels)\n",
    "print(f\"Modularity Score: {modularity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357bf07a",
   "metadata": {},
   "source": [
    "# More graph-based metrics\n",
    "### Conductance : Measures cluster boundary quality (lower = tighter clusters)\n",
    "\n",
    "Formula: boundary_edges / (internal_edges + boundary_edges)\n",
    "- Interpretation: < 0.3 is good, < 0.1 is excellent\n",
    "\n",
    "### Coverage: Fraction of edges within clusters (higher = better)\n",
    "\n",
    "Formula: internal_edges / total_edges\n",
    "- Interpretation: > 0.7 is good, > 0.9 is excellent\n",
    "\n",
    "\n",
    "### Internal Edge Density: Average density within clusters\n",
    "\n",
    "Shows how tightly connected nodes are within their clusters\n",
    "Higher values indicate more cohesive clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18aa4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conductance(graph, labels):\n",
    "    \"\"\"Compute conductance for each cluster (lower is better)\"\"\"\n",
    "    node_list = list(graph.nodes())\n",
    "    conductances = []\n",
    "    \n",
    "    for label in np.unique(labels):\n",
    "        cluster_nodes = set([node_list[i] for i, l in enumerate(labels) if l == label])\n",
    "        \n",
    "        # Count edges within and across cluster boundary\n",
    "        internal_edges = 0\n",
    "        boundary_edges = 0\n",
    "        \n",
    "        for u, v in graph.edges():\n",
    "            if u in cluster_nodes and v in cluster_nodes:\n",
    "                internal_edges += 1\n",
    "            elif u in cluster_nodes or v in cluster_nodes:\n",
    "                boundary_edges += 1\n",
    "        \n",
    "        # Conductance = boundary / min(internal, external)\n",
    "        if internal_edges + boundary_edges > 0:\n",
    "            conductance = boundary_edges / (internal_edges + boundary_edges)\n",
    "        else:\n",
    "            conductance = 1.0\n",
    "        \n",
    "        conductances.append(conductance)\n",
    "    \n",
    "    return np.mean(conductances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9ad6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coverage(graph, labels):\n",
    "    \"\"\"Compute coverage: fraction of edges within clusters\"\"\"\n",
    "    node_list = list(graph.nodes())\n",
    "    total_edges = graph.number_of_edges()\n",
    "    internal_edges = 0\n",
    "    \n",
    "    for u, v in graph.edges():\n",
    "        u_idx = node_list.index(u)\n",
    "        v_idx = node_list.index(v)\n",
    "        if labels[u_idx] == labels[v_idx]:\n",
    "            internal_edges += 1\n",
    "    \n",
    "    return internal_edges / total_edges if total_edges > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef9c9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_density_ratio(graph, labels):\n",
    "    \"\"\"Ratio of internal to external edge density\"\"\"\n",
    "    node_list = list(graph.nodes())\n",
    "    \n",
    "    internal_density = []\n",
    "    for label in np.unique(labels):\n",
    "        cluster_nodes = [node_list[i] for i, l in enumerate(labels) if l == label]\n",
    "        if len(cluster_nodes) > 1:\n",
    "            subgraph = graph.subgraph(cluster_nodes)\n",
    "            internal_density.append(nx.density(subgraph))\n",
    "    \n",
    "    return np.mean(internal_density) if internal_density else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b2cf6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conductance: 0.6664031386938685\n",
      "Coverage: 0.5119453924914675\n",
      "Edge Density Ratio: 0.5967643467643468\n"
     ]
    }
   ],
   "source": [
    "conductance = compute_conductance(graph, labels)\n",
    "coverage = compute_coverage(graph, labels)\n",
    "edge_density = compute_edge_density_ratio(graph, labels)\n",
    "\n",
    "print(f\"Conductance: {conductance}\")\n",
    "print(f\"Coverage: {coverage}\")\n",
    "print(f\"Edge Density Ratio: {edge_density}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2194c4d6",
   "metadata": {},
   "source": [
    "## Pairwise Comparisons\n",
    "\n",
    "Using normalised mutual information and adjusted random index\n",
    "\n",
    "### Normalized Mutual Information (NMI): Measures agreement between two clusterings\n",
    "\n",
    "Range: 0-1 (1 = perfect agreement)\n",
    "\n",
    "Use: Compare how similar different methods' results are\n",
    "\n",
    "### Adjusted Rand Index (ARI):\n",
    "\n",
    "Range: -1 to 1 (1 = perfect agreement, 0 = random)\n",
    "\n",
    "Adjusts for chance agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d4907278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
